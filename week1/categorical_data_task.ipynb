{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice assignment: Handling categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dZEp05rN8tN"
   },
   "source": [
    "In this programming assignment, you are going to work with the following dataset from Kaggle:\n",
    "\n",
    "https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists\n",
    "\n",
    "This assignment is graded by your `submission.json`.\n",
    "\n",
    "The cell below creates a valid `submission.json` file, fill your answers in there. \n",
    "\n",
    "You can press \"Submit Assignment\" at any time to submit partial progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submission.json\n"
     ]
    }
   ],
   "source": [
    "%%file submission.json\n",
    "{\n",
    "    \"q1\": 0,\n",
    "    \"q2\": 0,\n",
    "    \"q3\": 0,\n",
    "    \"q4\": 0,\n",
    "    \"q5\": 0,\n",
    "    \"q6\": 0,\n",
    "    \"q7\": 0,\n",
    "    \"q8\": 0,\n",
    "    \"q9\": 0,\n",
    "    \"q10\": 0,\n",
    "    \"q11\": 0,\n",
    "    \"q12\": 0,\n",
    "    \"q13\": 0,\n",
    "    \"q14\": 0,\n",
    "    \"q15\": 0,\n",
    "    \"q16\": \"\",\n",
    "    \"q17\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset description\n",
    "\n",
    "Suppose that a company working with Big Data and Data Science wants to hire data scientists among people who have successfully passed some courses conducted by the company. Many people sign up for their training. The company wants to focus of the candidates who really want to work for them after training. Information related to demographics, education, experience is provided by the candidates via a sign-up form.\n",
    "\n",
    "This dataset is designed to understand the factors that lead a person to leave current job which is useful for HR researches. Based on the provided data, you are going to predict whether a candidate is looking for a job change.\n",
    "\n",
    "The whole data is divided into train and test parts. Data contains several categorical features – they need to be encoded.\n",
    "\n",
    "## Feature description\n",
    "\n",
    "- `enrollee_id`: Unique ID for a candidate\n",
    "- `city`: City code\n",
    "- `city_development_index`: Developement index of the city (scaled)\n",
    "- `gender`: Gender of a candidate\n",
    "- `relevent_experience`: Relevant experience of a candidate\n",
    "- `enrolled_university`: Type of University course enrolled in if any\n",
    "- `education_level`: Education level of a candidate\n",
    "- `major_discipline`: Education major discipline of a candidate\n",
    "- `experience`: Candidate total experience in years\n",
    "- `company_size`: Number of employees in a current employer's company\n",
    "- `company_type`: Type of a current employer\n",
    "- `last_new_job`: Difference in years between previous and current jobs\n",
    "- `training_hours`: training hours completed\n",
    "- `target`: 0 – Not looking for job change, 1 – Looking for a job change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-riW7UN88QB0"
   },
   "source": [
    "## 1\n",
    "\n",
    "Before modifying the features, let's study our dataframe a bit. First, call `.dtypes` for `train` dataframe. \n",
    "\n",
    "**q1:** How many columns in `train` contain data types different from numbers (ints and floats)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "id": "TLmsIbke6RTE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enrollee_id                 int64\n",
       "city                       object\n",
       "city_development_index    float64\n",
       "gender                     object\n",
       "relevent_experience        object\n",
       "enrolled_university        object\n",
       "education_level            object\n",
       "major_discipline           object\n",
       "experience                 object\n",
       "company_size               object\n",
       "company_type               object\n",
       "last_new_job               object\n",
       "training_hours              int64\n",
       "target                    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZmIR0DeNj-4"
   },
   "source": [
    "## 2\n",
    "\n",
    "**q2:** How many unique caterogies does a feature `'city'` contain in the train data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "id": "w1OFKy0S6SdX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.city.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zT4YJ6DaOjF7"
   },
   "source": [
    "## 3\n",
    "\n",
    "**q3:** How many features in train data contain missing values (NaN)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "id": "BaEwQjRg6Tj_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enrollee_id                  0\n",
       "city                         0\n",
       "city_development_index       0\n",
       "gender                    4508\n",
       "relevent_experience          0\n",
       "enrolled_university        386\n",
       "education_level            460\n",
       "major_discipline          2813\n",
       "experience                  65\n",
       "company_size              5938\n",
       "company_type              6140\n",
       "last_new_job               423\n",
       "training_hours               0\n",
       "target                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxc_ZFXyQ2lc"
   },
   "source": [
    "## 4\n",
    "\n",
    "Some features have a relatively small number of missing values. For instance, `'experience'` has only 65 missing values in the train data. Replacing these missing values with a special category might introduce bias to the data. However, since the number of missing values is not so big, we might be OK with it.\n",
    "\n",
    "Replace missing values in `'experience'` feature with a category -1.\n",
    "\n",
    "**q4:** How many categories does this feature contain in the train data now?\n",
    "\n",
    "_(hint: you might want to use `fillna` function from `pandas` library in this task, but remember that it's not an in-place function by default. Or you can use `SimpleImputer` from `sklearn`)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "id": "dWv8Npro6VA1"
   },
   "outputs": [],
   "source": [
    "train.experience = train.experience.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.experience.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od7G5iQ2ZVNB"
   },
   "source": [
    "## 5\n",
    "\n",
    "`'education_level'` is an example of an ordinal feature. Sure, we can define an order on it. For instance, `'High School'` is \"bigger\" than `'Primary School'`, and `'Phd'` is \"bigger\" than '`Graduate'`. We can encode this feature with integer numbers in a correct order.\n",
    "\n",
    "In this task, apply a correct mapping for the values of `'education_level'` feature. The mapping should be the following:\n",
    "\n",
    "* `'Primary School'` -> 0\n",
    "* `'High School'` -> 1\n",
    "* `'Graduate'` -> 2\n",
    "* `'Masters'` -> 3\n",
    "* `'Phd'` -> 4\n",
    "\n",
    "At the same time, impute missing values in `'education_level'` with a new category -1. So another part of the mapping would be:\n",
    "\n",
    "* `np.nan` -> -1\n",
    "\n",
    "**q5:** What will be the mean value of this feature in the train data after the encoding? Provide the answer, rounded to the nearest TWO decimal places (e.g. 12.3456789 -> 12.35).\n",
    "\n",
    "_(hint: you might want to use `map` function from `pandas` in this task)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "BKDebpQX6WL7"
   },
   "outputs": [],
   "source": [
    "map_education = {\n",
    "    'Primary School': 0,\n",
    "    'High School': 1,\n",
    "    'Graduate': 2,\n",
    "    'Masters': 3,\n",
    "    'Phd': 4,\n",
    "    np.nan: -1\n",
    "}\n",
    "\n",
    "\n",
    "train.education_level = train.education_level.apply(lambda x: map_education[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.061384278108362"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['education_level'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAi9S-YBrOlY"
   },
   "source": [
    "## 6\n",
    "\n",
    "Feature `'relevent_experience'` is an example of a binary feature. You can also check that it has no missing values, which makes its encoding pretty easy.\n",
    "\n",
    "Encode this feature with the following mapping:\n",
    "\n",
    "*   `\"No relevent experience\"` -> 0\n",
    "*   `\"Has relevent experience\"` -> 1\n",
    "\n",
    "**q6:** What will be the mean value of this feature in the train data after the encoding? Provide the answer, rounded to the nearest TWO decimal places (e.g. 12.3456789 -> 12.35)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "id": "l5uTrVl36XjM"
   },
   "outputs": [],
   "source": [
    "map_relevent_experience = {\n",
    "    \"No relevent experience\": 0,\n",
    "    \"Has relevent experience\": 1\n",
    "}\n",
    "\n",
    "train.relevent_experience = train.relevent_experience.apply(lambda x: map_relevent_experience[x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7199081323728991"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['relevent_experience'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lv-zgcIDtzqR"
   },
   "source": [
    "## 7\n",
    "\n",
    "In our case, `'gender'` is an example of a nominal feature (notice that it is not a binary feature cause it contains three categories + NaNs). We will use One-Hot encoding to encode it. There are several options how to do it in practice: for example, to use `get_dummies` function from `pandas` or `OneHotEncoder` from `sklearn.preprocessing`. Here, we will go with the first option.\n",
    "\n",
    "If you want to encode a whole dataset with One-Hot encoding, you can directly pass it into the discussed methods. But here we want to encode only one feature. We suggest to do it in four steps:\n",
    "\n",
    "1. Obtain a One-Hot encoding dataframe for the feature `'gender'`: apply `pd.get_dummies` to this feature. As a result, you should obtain a dataframe with three features (three different categories for gender). Don't include `'NaN'` feature - it will already be included as the encoding (0, 0, 0).\n",
    "2. Change the column names of this dataframe, so that a feature `'<category_name>'` will become `'gender-<category_name>'`. Of course, it is not a necessary step in general. However, it probably would be more convenient to work with a full dataframe if we remember that these One-Hot encoded features originally came from the feature `'gender'`. On a technical side, you can perform it by changing `df_gender.columns` list.\n",
    "3. Concatenate original and new dataframes. You can do it by calling `pd.concat` function. Don't forget to set `'axis'` parameter, so that you concatenate dataframes by columns, not by rows. As a result of this step, you should obtain a new `train` dataframe with three new columns named like `'gender-<category_name>'`.\n",
    "4. Finally, drop `'gender'` feature because we have already encoded it.\n",
    "\n",
    "As a result of these four steps, you should obtain a new version of `train` dataframe, with dropped `'gender'` feature and three new features named like `'gender-<category_name>'`. A total number of columns by this time should be equal to 16.\n",
    "\n",
    "**q7:** What is the total number of zero values which appear in the columns starting with `'gender-'`?\n",
    "\n",
    "_Example: suppose that you obtain the following dataframe:_\n",
    "\n",
    "| gender-category1 | gender-category2   | gender-category3|\n",
    "|------|------|------|\n",
    "|   0  | 1| 0|\n",
    "|0|0|0|\n",
    "\n",
    "_Then the answer for the question above should be 5._\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "id": "7IGy0CV96Y8Q"
   },
   "outputs": [],
   "source": [
    "dum_features = pd.get_dummies(train.gender)\n",
    "dum_features.columns = [f'gender-{col}' for col in dum_features.columns]\n",
    "dum_features\n",
    "\n",
    "train = pd.concat([train.drop(columns=['gender']), dum_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42824"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dum_features == 0).sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJx2G8ZQv_Hy"
   },
   "source": [
    "## 8\n",
    "\n",
    "Perform One-Hot encoding for the feature `'enrolled_university'`, using the similar procedure as in the previous task:\n",
    "\n",
    "1. Obtain a One-Hot encoding dataframe for `'enrolled_university'`.\n",
    "2. Rename its columns.\n",
    "3. Concatenate original and One-Hot encoding dataframes.\n",
    "4. Drop `'enrolled_university'` column.\n",
    "\n",
    "A total number of columns by this time should be equal to 18.\n",
    "\n",
    "**q8:** What is the total number of zero values which appear in the columns starting with `'enrolled_university-'`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "id": "TrHkhli66aPP"
   },
   "outputs": [],
   "source": [
    "dum_features = pd.get_dummies(train.enrolled_university)\n",
    "dum_features.columns = [f'enrolled_university-{col}' for col in dum_features.columns]\n",
    "dum_features\n",
    "\n",
    "train = pd.concat([train.drop(columns=['enrolled_university']), dum_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38702"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dum_features == 0).sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBervQFY9N7K"
   },
   "source": [
    "## 9\n",
    "\n",
    "Encode feature `'city'` using frequency encoding. You should map each category `'city_i'` to its count (a total number of observations in `train` with `city == 'city_i'`). Save this mapping, since later you would apply the same mapping to the test data.\n",
    "\n",
    "As a result of this task, feature `'city'` should be encoded with category counts in `train`. \n",
    "\n",
    "**q9:** What will be the mean value of this feature in the train data after the encoding? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "id": "Co-kiLuc6bO6"
   },
   "outputs": [],
   "source": [
    "map_city = train.groupby('city').size()\n",
    "\n",
    "train.city = train.city.apply(lambda x: map_city[x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1709.7964296899468"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.city.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksJw_j-M9XCC"
   },
   "source": [
    "## 10\n",
    "\n",
    "Encode feature `'last_new_job'` with target encoding with no modifications. First, impute missing values in this feature with a new category `'-1'`. Then, map each category of `'last_new_job'` to the mean target value of the observations with the corresponding category. Save this mapping, since later you would apply the same mapping to the test data.\n",
    "\n",
    "**q10:** What will be the maximum value of this feature in the train data after the encoding? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.35).\n",
    "\n",
    "_(hint: you might want to use `groupby` function from `pandas` in this task)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "id": "tmZgKA1_6caQ"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "train.last_new_job = train.last_new_job.fillna(-1)\n",
    "map_last_new_job = train.groupby('last_new_job').target.mean()\n",
    "\n",
    "train.last_new_job = train.last_new_job.apply(lambda x: map_last_new_job[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3640661938534279"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train.last_new_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyIVTPEY9eLb"
   },
   "source": [
    "## 11\n",
    "\n",
    "Encode feature `'experience'` with M-estimate encoding. Map each category of `'experience'` according to the following formula:\n",
    "\n",
    "$$\n",
    "\\hat{x_{ij}} = \\frac{\\text{target}\\left(j, x_{ij}\\right) + m\\times y_{\\text{mean}}}{\\text{count}\\left(j, x_{ij}\\right) + m}\\quad,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $x_{ij}$ is a category being encoded,\n",
    "* $\\hat{x_{ij}}$ is its corresponding M-estimate encoding value,\n",
    "* $\\text{count}\\left(j, x_{ij}\\right)$ is a total number of times $x_{ij}$ appeared in `train`,\n",
    "* $\\text{target}\\left(j, x_{ij}\\right)$ is a mean target value of the observations with the corresponding category,\n",
    "* $m$ is a parameter.\n",
    "\n",
    "In this task, set $m = 0.5$. \n",
    "\n",
    "**q11:** What will be the maximum value of this feature in the train data after the encoding? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "id": "cbihpCuH6dxL"
   },
   "outputs": [],
   "source": [
    "count_exp = train.groupby('experience').count()['target']\n",
    "target_exp = train.groupby('experience').sum()['target']\n",
    "y_mean = train.target.mean()\n",
    "m = 0.5\n",
    "train.experience = train.experience.map((target_exp+m*y_mean)/(count_exp+m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4538271268239785"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train.experience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0eCGRxLLJ-Y"
   },
   "source": [
    "## 12\n",
    "\n",
    "Encode feature `'major_discipline'` with Leave-One-Out encoding. Remember that this technique is similar to target encoding, but here while computing the encoding for a particular observation, we exclude it from the target encoding formula. Therefore a category $x_{ij}$ for the $i$-th observation will be encoded according to the following formula:\n",
    "\n",
    "$$\n",
    "\\hat{x_{ij}} = \\frac{\\text{target}\\left(j, x_{ij}\\right) - y_i}{\\text{count}\\left(j, x_{ij}\\right) - 1}\\quad,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\hat{x_{ij}}$ is its corresponding M-estimate encoding value,\n",
    "* $\\text{count}\\left(j, x_{ij}\\right)$ is a total number of times $x_{ij}$ appeared in `train`,\n",
    "* $\\text{target}\\left(j, x_{ij}\\right)$ is a mean target value of the observations with the corresponding category,\n",
    "* $y_i$ is a target value of the $i$-th observation\n",
    "\n",
    "For example, suppose that you have the following train data:\n",
    "\n",
    "|feature|target|\n",
    "|-|-|\n",
    "|A|1|\n",
    "|A|0|\n",
    "|B|1|\n",
    "|B|0|\n",
    "|A|0|\n",
    "\n",
    "Then you obtain the following Leave-One-Out encoding:\n",
    "\n",
    "|feature|feature_loo_encoded|\n",
    "|-|-|\n",
    "|A|0.0|\n",
    "|A|0.5|\n",
    "|B|0.0|\n",
    "|B|1.0|\n",
    "|A|0.5|\n",
    "\n",
    "It is very important to notice that in this method the same category can be encoded differently for different observations. Thus, after encoding the train part of data, you should create a mapping which will help to encode the test data. In order to do this, simply average train encoding values within each category to obtain the final encoding. For instance, suppose that you obtain the following train dataframe:\n",
    "\n",
    "|feature|feature_loo_encoded|\n",
    "|-|-|\n",
    "|A|0.2|\n",
    "|A|0.6|\n",
    "|B|0.3|\n",
    "|B|0.7|\n",
    "|A|0.4|\n",
    "\n",
    "Then, for the test data, you should obtain the following mapping:\n",
    "\n",
    "* A -> 0.4 (because 0.4 is a mean value of the encoded values for the category A)\n",
    "* B -> 0.5 (because 0.5 is a mean value of the encoded values for the category B)\n",
    "\n",
    "Don't impute any missing values in this task. After completing this task, drop the feature `'major_discipline'` and rename `'major_discipline_loo_encoded'` to `'major_discipline'`.\n",
    "\n",
    "**q12:** What will be the maximum value of the encoding values for `'major_discipline'` for the TEST data in the mapping described above? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "id": "sV6QygUw6fcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2677165354330709"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d = {'feature': ['A', 'A', 'B', 'B', 'A'], 'target': [1, 0, 1, 0, 0]}\n",
    "# df = pd.DataFrame(data=d)\n",
    "\n",
    "\n",
    "\n",
    "train['major_discipline_loo_encoded']=(train['major_discipline'].map(train.groupby('major_discipline').sum()['target']) - train['target'])/(train['major_discipline'].map(train.groupby('major_discipline').count()['target'])-1)\n",
    "# train.drop(columns=['major_discipline'], inplace=True)\n",
    "# train.rename(columns={'major_discipline_loo_encoded':'major_discipline'})\n",
    "mapping = train.groupby('major_discipline')['major_discipline_loo_encoded'].mean()\n",
    "\n",
    "mapping\n",
    "# test.major_discipline = test.major_discipline.apply(lambda x: mapping[x])\n",
    "max(test.major_discipline.map(mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQmH1y-d9qOW"
   },
   "source": [
    "## 13\n",
    "\n",
    "Encode feature `'company_size'` with Catboost encoding. The technique was described in the lecture. Here, for the sake of simplicity, let's use the implementation from `category_encoders` library.\n",
    "\n",
    "As you may remember, Catboost encoding depends on how the data is ordered. Normally, you should shuffle the data one time or even several times. In this task, we will assume that the data has already been shuffled, so you don't need to shuffle it again.\n",
    "\n",
    "Take `CatBoostEncoder` and use the default values for all its parameters, except `handle_missing` - set it to `'return_nan'` so that your encoder don't do anything with missing values. Fit it on the `'company_size'` (train data) and `'target'` and transform this feature. Save the encoder - it will be used later to transform this feature in test data.\n",
    "\n",
    "Don't impute any missing values in this task.\n",
    "\n",
    "**q13:** What will be the most popular value of this feature in the train data after the encoding? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "id": "PfmCVKRx6hHl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.249348    8\n",
       "0.124674    6\n",
       "0.249870    5\n",
       "0.224935    5\n",
       "0.178478    5\n",
       "           ..\n",
       "0.173223    1\n",
       "0.138409    1\n",
       "0.148918    1\n",
       "0.174829    1\n",
       "0.176857    1\n",
       "Name: company_size, Length: 12567, dtype: int64"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "\n",
    "cbe = CatBoostEncoder(handle_missing='return_nan')\n",
    "\n",
    "train.company_size = cbe.fit_transform(train.company_size, train.target)\n",
    "\n",
    "train.company_size.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBE4sCLL9uV4"
   },
   "source": [
    "## 14\n",
    "\n",
    "Encode feature `'company_type'` with Weight of Evidence (WoE) encoding. A category $x_{ij}$ will be encoded according to the following formula:\n",
    "\n",
    "$$\n",
    "\\hat{x_{ij}} = \\ln\\left(\\frac{\\mathbb{P}\\left(x_{ij}\\mid y=1\\right)}{\\mathbb{P}\\left(x_{ij}\\mid y=0\\right)}\\right)\\quad.\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left(x_{ij}\\mid y=1\\right) = \\frac{\\text{count}\\left(y=1\\mid x_{ij}\\right)}{\\text{count}\\left(y=1\\right)}\n",
    "$$\n",
    "$$\n",
    "\\mathbb{P}\\left(x_{ij}\\mid y=0\\right) = \\frac{\\text{count}\\left(y=0\\mid x_{ij}\\right)}{\\text{count}\\left(y=0\\right)}\n",
    "$$\n",
    "\n",
    "The notation means the following:\n",
    "\n",
    "* $\\text{count}\\left(y=1\\mid x_{ij}\\right)$ denotes the number of observations with the category $x_{ij}$ where the target value is equal to $1$;\n",
    "* $\\text{count}\\left(y=0\\mid x_{ij}\\right)$ denotes the same but for the target value $0$;\n",
    "* $\\text{count}\\left(y=1\\right)$ denotes the number of observations with the target value equal to $1$;\n",
    "* $\\text{count}\\left(y=0\\right)$ denotes the same but for the target value $0$.\n",
    "\n",
    "\n",
    "For example, suppose that you have the following train data:\n",
    "\n",
    "|feature|target|\n",
    "|-|-|\n",
    "|A|1|\n",
    "|A|0|\n",
    "|B|1|\n",
    "|B|0|\n",
    "|A|0|\n",
    "\n",
    "Then you obtain the following WoE encoding mapping:\n",
    "\n",
    "* A -> $\\ln\\left(\\frac{\\frac{1}{2}}{\\frac{2}{3}}\\right) \\approx -0.288$ \n",
    "* B -> $\\ln\\left(\\frac{\\frac{1}{2}}{\\frac{1}{3}}\\right) \\approx 0.405$ \n",
    "\n",
    "Don't impute any missing values in this task.\n",
    "\n",
    "**q14:** What will be the most popular value of this feature in the train data after the encoding? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "id": "A52TWXmH6igK"
   },
   "outputs": [],
   "source": [
    "# map_company_type = # your code here\n",
    "# # your code here\n",
    "\n",
    "\n",
    "map_company_type = {}\n",
    "for feature in train.company_type.unique():\n",
    "    if feature is not np.nan:\n",
    "        count_y1_x = len(train[(train.target == 1) & (train.company_type==feature)])\n",
    "        count_y0_x = len(train[(train.target == 0) & (train.company_type==feature)])\n",
    "        count_y1 = len(train[train.target == 1])\n",
    "        count_y0 = len(train[train.target == 0])\n",
    "    else:\n",
    "        count_y1_x = len(train[(train.target == 1) & (train.company_type.isnull())])\n",
    "        count_y0_x = len(train[(train.target == 0) & (train.company_type.isnull())])\n",
    "        count_y1 = len(train[train.target == 1])\n",
    "        count_y0 = len(train[train.target == 0])\n",
    "        \n",
    "    map_company_type[feature] = round(np.log(((count_y1_x/count_y1)/(count_y0_x/count_y0))), 5)\n",
    "\n",
    "    \n",
    "train.company_type = train.company_type.apply(lambda x: map_company_type[x])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pvt Ltd': -0.40878,\n",
       " 'Early Stage Startup': -0.07548,\n",
       " nan: 0.64752,\n",
       " 'NGO': -0.37293,\n",
       " 'Funded Startup': -0.71436,\n",
       " 'Public Sector': -0.16418,\n",
       " 'Other': -0.0524}"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_company_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.40878    9817\n",
       " 0.64752    6140\n",
       "-0.71436    1001\n",
       "-0.16418     955\n",
       "-0.07548     603\n",
       "-0.37293     521\n",
       "-0.05240     121\n",
       "Name: company_type, dtype: int64"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.company_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFwADkfU9wxD"
   },
   "source": [
    "## 15\n",
    "\n",
    "We have encoded all categorical features. Next, we drop `'enrollee_id'` because it is not a representative feature. After this, we split train part of data into the dataframe which contains only features (without target) and the target array.\n",
    "\n",
    "Before training the models, we should impute the remaining missing values. You might have noticed that we didn't impute missing values for the features `'major_discipline'`, `'company_size'` and `'company_type'`. This is because the number of missing values in these features is relatively big (you can check it yourself). In practice, you might just create a special category (like `'-1'`) for each of these features before the encoding. However, in this task, let's perform the imputation using KNN approach - where we impute missing values by looking at the similar observations.\n",
    "\n",
    "Import `KNNImputer` from `sklearn`. It works only with the dataset with numbers - this is why we didn't run it before the encoding. Set `n_neighbors=3`, and let other parameters have the default values. Fit it on the train dataframe with features, and then transform it. Notice that after the transformation we will obtain `numpy.array` - make `pandas.DataFrame` out of it with the same columns as before.\n",
    "\n",
    "Save the KNN imputer - you will need it to process the test data. Check that there are no missing values in the train data anymore.\n",
    "\n",
    "**q15:** What is the mean value of the `'company_size'` feature in the train data after the imputation? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 632,
     "status": "ok",
     "timestamp": 1610996515882,
     "user": {
      "displayName": "Evgeny Kovalev",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhapCcPAzYLJE2xqpH7xgS8qtR8bZrkCgbHm27oLg=s64",
      "userId": "07828702499945486090"
     },
     "user_tz": -180
    },
    "id": "eCz_Pr_xoa82",
    "outputId": "1457ef28-3783-488e-ea52-fc79f2e6f893"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19158, 16), (19158,))"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train.drop(['enrollee_id', 'target'], axis=1)\n",
    "y_train = train['target']\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "id": "YvfwLqvS6kKr"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'STEM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5y/shwh_m1j3yz27cc3yvjbrbfh0000gn/T/ipykernel_6839/3862316086.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNNImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimputer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNNImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/impute/_knn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \"Expected n_neighbors > 0. Got {}\".format(self.n_neighbors))\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         X = self._validate_data(X, accept_sparse=False, dtype=FLOAT_DTYPES,\n\u001b[0m\u001b[1;32m    184\u001b[0m                                 \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                                 copy=self.copy)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'no_validation'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    671\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, like)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNpDtype\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1993\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m     def __array_wrap__(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, like)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'STEM'"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "imputer.fit_transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2T2a78d90mJ"
   },
   "source": [
    "## 16\n",
    "\n",
    "Finally, train a Random Forest classifier from `sklearn` on the train data. Set `n_estimators=500`, `max_depth=8`, `random_state=13`, and let other parameters have the default values. Check feature importances. \n",
    "\n",
    "**q16:** What is the name of the most important feature for this model? Provide the name of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4_1694F6l0Q"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUydmtoi92d8"
   },
   "source": [
    "## 17\n",
    "\n",
    "In this last task, process the test data so that it is possible to make Random Forest predictions on for it. Perform the similar operations as for the train data, but remember that now you work with test observations and therefore all operations are in the inference mode.\n",
    "\n",
    "1. (task 4) Feature `'experience'`: impute missing values by -1. \n",
    "2. (task 5) Feature `'education_level'`: perform ordinal encoding mapping.\n",
    "3. (task 6) Feature `'relevent_experience'`: perform binary mapping.\n",
    "4. (task 7) Feature `'gender'`: perform One-Hot encoding and obtain three new features starting with `'gender-'`. Drop `'gender'` feature.\n",
    "5. (task 8) Feature `'enrolled_university'`: perform One-Hot encoding and obtain three new features starting with `'enrolled_university-'`. Drop `'enrolled_university'` feature.\n",
    "6. (task 9) Feature `'city'`: perform frequency encoding mapping.\n",
    "7. (task 10) Feature `'last_new_job'`: impute missing values by -1 and perform target encoding mapping.\n",
    "8. (task 11) Feature `'experience'`: perform M-estimate encoding mapping.\n",
    "9. (task 12) Feature `'major_discipline'`: perform Leave-One-Out encoding mapping.\n",
    "10. (task 13) Feature `'company_size'`: perform Catboost encoding mapping.\n",
    "11. (task 14) Feature `'company_type'`: perform WoE encoding mapping.\n",
    "12. (task 15) Split `test` into `X_test` (a dataframe with no `'enrollee_id'` and `'target'`) and `y_test` (an array with target values). Impute missing values by using KNN imputer which you used before (now only in a transform mode).\n",
    "\n",
    "As a result of the operations described above, you should obtain `X_test` which is a `pandas.DataFrame` with a shape (2129, 16). Calculate the predictions of the trained Random Forest model on it. Check the accuracy of the predictions on test data.\n",
    "\n",
    "Then calculate the predictions of the same model on `X_train` and check the accuracy there. Compare the accuracies on train and test data. Do you notice something? What, in your opinion, caused such difference in the accuracies?\n",
    "\n",
    "**q17:** As a result of this task, provide the accuracy for the test data, rounded to the nearest TWO decimal places (e.g. 12.3456789 -> 12.35)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "veC-y23Fwe8L"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 752,
     "status": "ok",
     "timestamp": 1610996545175,
     "user": {
      "displayName": "Evgeny Kovalev",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhapCcPAzYLJE2xqpH7xgS8qtR8bZrkCgbHm27oLg=s64",
      "userId": "07828702499945486090"
     },
     "user_tz": -180
    },
    "id": "4kK-7S1Q16lp",
    "outputId": "b5b5ea5a-e883-4349-b9d8-fe0c91eae5a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2129, 16)"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yM1CI29I6rv5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO77HBmz1/PpQ55VCcYPEhx",
   "collapsed_sections": [],
   "name": "Week1_practice.ipynb",
   "provenance": [
    {
     "file_id": "1WztG2ZwKA0bx5LJIEQNQH23OULDBBmt8",
     "timestamp": 1610996564372
    }
   ]
  },
  "coursera": {
   "schema_names": [
    "categorical-data-task-week-1-1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
